{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the mnist dataset\n",
    "With the purpose of distinguishing 3s from 7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import matplotlib\n",
    "matplotlib.rc('image', cmap='Greys')\n",
    "\n",
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAO6klEQVR4nO1c22/aZh9+bIPPNphDCiRlSUuWptEqdVI1bdIqbZOm3ex6/+Huvt3scheRtkrVurVNu6rrIW1SkgDBHAM22GD7u5jeV5BDm7UkuB2PhBLJBoMfv7/j83uZIAgCzDBVsNP+AjPMSAgFZiSEADMSQoAZCSHAjIQQYEZCCDAjIQSYkRACzEgIAWYkhAAzEkKAGQkhwIyEEGBGQggwIyEEmJEQAsxICAFmJIQAMxJCgBkJIUBkmhd/ndCDYZhz/CbTxcRICIIAQRBgOByi3+9jOBwCAFiWRRAEcF0XrusiCAL4vg/f9zEYDNBqteA4DkRRhKIo4DiOvt4EhmHGzpdlGdFoFCzL0r/vAyZCwuiN7ff7KBaLaLfbYFkWHMchCAKYpolKpYLBYADHcTAcDmGaJu7fv49KpYKFhQVcunQJgiBAVVWoqvrG1cAwDBRFgaZpUBQFhUIBiUQCoihC07T/FgkA4HkePM+D67pot9uo1Wr0iQyCAJVKBeVyGcPhEI7jYDAYwDRNPHv2DKZpwrZt8DwPWZbpjT0NCaqqotPpQFVVpFIpCIIA3/chiiI9h3zO4f/DAmYSCrzBYID9/X3UajXs7+9jfX0dlUoFLMtSc2RZFtrtNjzPw3A4hOd5aLfbePXqFSzLgqZpSCaTEAQBoihCkqRT3ShJkiAIAgRBQDqdhq7r0HUdCwsL0HUdhmEgnU6D53noug5VVcGyLCKRyKlM3nlgIivB8zzs7Ozgjz/+QLFYxM8//4xisTh2ju/7RxwxMWNBEKDVaqHdbr/Vk0rOjUQiYFkWqqoin89D13UUCgVcu3YNhmFgdXUVFy9eRDQapf4kDJiYT7BtG41GA+12G5ZlwXGcE88nZoFhGPA8fyrbzTAMIpEIGIahpo9cm5Dpui583wcA1Ot1uK4LwzCwv7+Pfr+PhYUFDAYDujrDgomthK2tLTx69AjVahW2bZ94LsMwEAQBPM9DFEUkk0nIsvzGawiCQG2+ZVmo1+vUyZPIq16vo9vtwnEc1Go1NJtNtFotvHr1CrFYDAzDIJVKQZZl8Dw/iZ8+EUxsJZimib///huWZaHf7594Lnn6BUGAoiiYn5+HrutvvIYsy5ibm4MkSWi329A0DYPBAJZlwbZt2LaNbreLbreL4XCIg4MDAECz2cSrV68gyzKuXr2KdrsNAHQlhQETIYFlWSwuLuKzzz5Dr9dDqVSCbduIRCIQRXHMvnMcB1VVIQgCJElCJpM51UoQRRGJRAKSJOHg4ADNZpOuhH6/T1eAZVnU+Y+aHN/3YVkWms0mGIZBJpOZxE+fCCZCAs/z+PLLL3HlyhX0+32YpkkjHsMwEIlEjpzPcRx4noemaUeOHweSc7AsC9/36U0eDAYYDodoNBrgOA6tVgu9Xg8HBwcYDAb0/Z7noVqtYmNjA9lsFvPz84jH45P4+e+MiZDAcRwMw4CiKHAcB4ZhwHEcqKoKXdfHbjLDMDR0JUS8S5RCnLSiKEilUuB5Hp7nHXH2QRCg0+mg0WhAFEWa0YcBEyGBRC4A6NM6HA5p/D5qjkYjI5Zl3ylpCoKAmqB6vY5ms4lutwvXdcdsPsuy4Hke2WwWKysryGazkCTp7X/whDExEniep9nxqI0/KfycROYaBAG63S52d3dRqVRQKpXQaDRoCYVcn+M4CIKAixcv4saNG7TMERZMrGwxWhI4a5C8wPM8av/r9TosyzpiZqLRKCRJgqqq0DQNqqpCkqTQJGrAlEvZ/xYkMet0Otjb20O328WDBw/w119/odVqYXNz88h7rly5gm+++QapVAo3b95EMpkMVckCeI9IIAQEQYB6vY719XXs7+/j4cOHuH37NhzHQa/XG3sPwzBYW1vDDz/8gHQ6jUQiQauzYSrghZqE0doSCUtJ4c80TdRqNdRqNdi2TY8B/9SQSC6SSCSgaRpkWaZljzARAIScBN/3aTZsWRZ2d3fRarWwsbGB9fV1mKaJRqMBx3Hg+z4tymWzWXz11Ve4cOECPv/8c1qqICSEDaEmgRQGm80m6vU67ty5g/39fTx//hz3799Ht9sdO5/jODAMA8Mw8Mknn2BlZQWXLl2Coiih7rSFkoTRBlG5XMbLly9RrVZRLBbRbDZhmuaJtR+Sf5CySFhN0ChCR8JoAtZsNvHTTz/hl19+Qa/XQ6vVguu66PV6ry2VkwaOrutjyWJYiQgdCQDG4v+XL1/izz//pM75NOA4DtFo9IgJIgW9sJEROhJIK7RcLqNSqaDb7dLQ9DTv9X0flUoFt2/fRrFYRC6Xw/LyMhUQEEXHqDKDdOSmhVCS0Gq18PDhQ1QqFVQqlVOvAHLe7u4ufvzxR/A8T0mQJAmXLl1CoVBALBbD0tISkskkeJ6HoigzEg7D8zzaMQP+iftJvnCaFTEYDNBoNOhncRwHXdchSRIURYFlWVQa43ke9RvTUmOEjgSGYaDrOtbW1pBKpWiDyLZtKhQbDodUSPYmkPyC53l0u12USiWIooiHDx8iFoshnU7j+vXriMfjiMfjtKxBCn/ngdCSkM/nEYvFsL29TZ30kydP0O120e/3MRgMTkWCbdvo9XpgGAa7u7tjvQyGYbC4uIjvvvsO8/PzuH79OhRFgSAIiEaj/10SgH+iG9J4yWQyqNfriMVi8DxvrHPmeR6CIKB/XdfFcDikUkziI0bNGMkviLlpt9uo1+uIRCIolUrI5/NQFAWqqp5bhj0R8dekQVqWpGFPnv5Go4Fer0eb+uRmk1exWESj0YBpmrSy+ibIsoxcLgdd1/Hxxx/j2rVrSCQSuHnzJpaXl9+58XQahHIlkBgfADRNAwB6wz3Pow1+4sCJydnY2MD29jZUVcXm5uapSLBtm5bAK5UKms0mUqkUCoUCLl++fC5RUyhJOA6j4i9SmvA8D5FIBNFoFKIoolAoQJIkxONxKnXpdDpotVqUsNdl2kSvxDAMyuUyOp0OleeQh+Is8N6QQMTFpIV63CubzVK/sbq6ir29PWxtbeHevXtoNpsol8uoVqsnOvSDgwM8f/4ce3t7KBQKmJubQzwex/Ly8pkqM94bEk6jHSXNe0EQcPnyZRiGAd/3sbOzAwBUc3QSCYPBgJq6arWK7e1tZLNZ5PP5yf6YQ3hvSPg34DiOyitJuaLVauHu3bu4desWer0eOp3OkU4cge/7cBwH7XYbiqKcuVrvgySB53mk02kEQYBcLoe1tTW4rosLFy6g2+2iXq/j2bNnJ5IAgIoHBEEYE5GdBT5IEkZ1UJFIBDzPg+d5pFIpKIqCfr//WtNGfMyo+vssEc5W0wQxWhNSVRWZTAaZTOa1uiOGYejQSiKROJVM813wQa6EwyAkKIqCTCYDlmVfSwLLspBlmZJwluEpEAISRoc8SPlh9OklsspJYFSx8abx3UgkAlmWIcvymSdsUyWB3BTf99HpdFAqleC6LhRFgSRJ4HmeyuHfBYTobreLcrmMer2OTqdz7LmkuGcYBvL5POLx+JkPlEx9mJw4v1arhcePH8OyLMzNzSGZTNJC2ruQQJ540rsm0zvHZc5k9ZFVYBgGLeSdJaZCAmnQOI5DZxmKxSI2NzfhOA7VjMqy/Fbl5NEsenSQZGdnB41GA7Va7diwk1RvZVmmomFJkj48c0Sm/j3Pg2ma+N///ofNzU2YponHjx+D4zjMz88jl8tBFEUIgvBW1xgMBvB9H7VaDU+fPoVpmrh16xbu3r2LXq93bI6gKArS6TSSySSWlpaQTqcRjUY/vJUw6oQty8KLFy9w7949VKtV7O3tQRAEOI4DSZLeWrB1+BpbW1uoVqtUOj/aEDpulCsej1O5zFkTAEyBBM/zUKvVYJomNjc3sbe3h2q1CsuyqJki05jAeJx/XF1/NLqyLAuWZaHX62Fvbw8HBwd0qrTT6WBnZ+fYeWoSgV28eBFffPEFUqkUcrncufWZz52E4XCIJ0+e4Ndff0WpVMKDBw9QKpXokwv8U+PvdDo0PCSNleMaLKMzzFtbW3j69Cl2d3fx22+/YXt7G51Oh+4kQPoRoyADLhzHYXV1Fd9//z3m5uawsLDw4ZIwusVCt9tFr9cbc5JBEKDf71ObTabvTyKBTOWQPTWIXunFixfY2dmB67pwHOfEvIA4Y0EQEIvFkEgkEIvFjox5nSXOnQSWZZFOp7G0tASWZelGIASu6+LOnTvwfR+KoiCXy1EFxGEbTUxXv99Hp9PBxsYGisUiNXek+3aYgNEkMJfL4euvv0Ymk8Gnn36KhYUFKIpyrsPm504CUU0XCgW4rnskBxgOh7h16xZ+//13CIKAxcVFZDIZ8Dx/ZMyJhLn9fh+2bdMdY0ab/yd9BxL1LC4u4ttvv8XKygqNjEiZ4oNdCcQGa5oGTdMQi8WgqirdgoeEsOTVbDbpqC2ZMSAIggC9Xo+ar8Om7fB1yQ4yHMdB0zRIkoT5+XnMzc1B13V67Ly1qlMxR/F4nNr5GzduwDAM7O7u0mSNwPM8ul/FqFZoFKOlj5P21CCt0ZWVFaytrUGWZSwtLVEHfOXKFei6PtE61b/BVEggZmUwGOCjjz6iYeb29vYYCb7v0/0q3hajhcB8Po/V1VUYhoHr168jk8lAkiQkEompbjgylbIFUUIrioLl5WWIogie52HbNtUZkTyB2G7XdWHb9hFzQ46PrhDy2dFodGwTq6tXr2JlZQWxWAyGYdD257Sl8lMRf5Gwcjgc0oGParWKR48eod1uo1QqoVwuUyeuKAra7TZevHiBWq1GP4f0kmOx2JgZ0TQN+XwemqZB13W63UI2m0U6nabmiZi3ac+yTW0lEBtPJCw8z9Otc8jTScJZwzBQrVbRarWOlBkSiQSSyeRY1BSPx7G6ukqdfjweRzQaha7rUBRl6k/+YUxVBjmqEe31emg2m3BdF61WC81mk3a4RFGEbdsoFotjfQCGYRCLxaBp2hgJoigilUpRM0d8EBFyzUg4AaNli8P1HaIVOq7uc1xNaTS7Hj1+HrrSt0FoSPgv44NXW7wPmJEQAsxICAFmJIQAMxJCgBkJIcCMhBBgRkIIMCMhBJiREALMSAgBZiSEADMSQoAZCSHAjIQQYEZCCPB/VN5r+YJ/pakAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Path.BASE_PATH = path\n",
    "im3 = Image.open(path/'train'/'3'/'12.png')\n",
    "show_image(im3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataloader\n",
    "mnist = DataBlock(blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),\n",
    "                get_items = get_image_files,\n",
    "                splitter = GrandparentSplitter(),\n",
    "                get_y = parent_label)\n",
    "\n",
    "dls = mnist.dataloaders(path)\n",
    "xb, yb = first(dls.valid)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One batch contains 64 images, each of 1 channel, with 28×28 pixels\n",
    "\n",
    "A *channel* is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions `[channels, rows, columns]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, fastai puts data on the GPU - but I don't have any :-)\n",
    "xb,yb = to_cpu(xb),to_cpu(yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "simple_nn = nn.Sequential(\n",
    "    nn.Linear(28*28, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=30, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n",
    "Using **nn.Conv2d(in_channels, out_channels, kernel_size, more..)** instead of F.conv2d because the module automatically initializes the weights and biases (alternatively we would have to define them ourselves and pass them as arguments to F.conv2d)\n",
    "\n",
    "The parameters kernel_size, stride, padding, dilation can either be:\n",
    "- a single int – in which case the same value is used for the height and width dimension\n",
    "- a tuple of two ints – in which case, the first int is used for the height dimension, and the second int for the width dimension\n",
    "\n",
    "**Out channels:**\n",
    "\n",
    "The number of output channels, **also called features, is the number of filters that will be applied to the input image**. Each filter is a different set of weights and biases that will be learned by the CNN. The more filters we have, the more patterns we can detect, but the more computationally expensive our model will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_cnn = nn.Sequential(\n",
    "    #in_channels = 1, out_channels = 30, kernel size = 3x3\n",
    "    #padding = 1 means that the output will be the same size as the input\n",
    "    nn.Conv2d(1,30, kernel_size=3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    #in_channels = 30, out_channels = 1, kernel size = 3x3\n",
    "    nn.Conv2d(30,1, kernel_size=3,padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note here is that we didn't need to specify 28×28 as the input size. That's because a linear layer needs a weight in the weight matrix for every pixel, so it needs to know how many pixels there are, but a convolution is applied over each pixel automatically. The weights only depend on the number of input and output channels and the kernel size, as we saw in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_cnn(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output cannot be used for classification as we have 28 x 28 map of activations. One way to deal with this, is to add enough stride-2 convolutions (halves the size of the image) to reduce the size of the final map to 1 x 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to create each convolutional layer\n",
    "\n",
    "def conv(n_inputs, n_features, ks=3, activation=True):\n",
    "    res = nn.Conv2d(n_inputs, n_features, stride=2, kernel_size=ks, padding=ks//2)\n",
    "    if activation: \n",
    "        res = nn.Sequential(res, nn.ReLU())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to stride=2 the image size is reduced by half in each layer (rounded up if uneven)\n",
    "#At the same time, we increase the number of features (channels) by a factor of 2 to compensate for the loss of information\n",
    "\n",
    "simple_cnn = nn.Sequential(\n",
    "    conv(1, 4),            #14x14\n",
    "    conv(4, 8),            #7x7\n",
    "    conv(8, 16),           #4x4\n",
    "    conv(16, 32),          #2x2\n",
    "    conv(32, 2, activation=False), #1x1\n",
    "    nn.Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_cnn(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which the learner can now be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Sequential (Input shape: 64 x 1 x 28 x 28)\n",
       "============================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "============================================================================\n",
       "                     64 x 4 x 14 x 14    \n",
       "Conv2d                                    40         True      \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 8 x 7 x 7      \n",
       "Conv2d                                    296        True      \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 16 x 4 x 4     \n",
       "Conv2d                                    1168       True      \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 32 x 2 x 2     \n",
       "Conv2d                                    4640       True      \n",
       "ReLU                                                           \n",
       "____________________________________________________________________________\n",
       "                     64 x 2 x 1 x 1      \n",
       "Conv2d                                    578        True      \n",
       "____________________________________________________________________________\n",
       "                     64 x 2              \n",
       "Flatten                                                        \n",
       "____________________________________________________________________________\n",
       "\n",
       "Total params: 6,722\n",
       "Total trainable params: 6,722\n",
       "Total non-trainable params: 0\n",
       "\n",
       "Optimizer used: <function Adam at 0x7f6593cff1a0>\n",
       "Loss function: <function cross_entropy at 0x7f65bc47a2a0>\n",
       "\n",
       "Callbacks:\n",
       "  - TrainEvalCallback\n",
       "  - CastToTensor\n",
       "  - Recorder\n",
       "  - ProgressCallback"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the summary, the first layer has dim 64x1x28x28, the axis are batch_size, channels, height, width - often written NCHW (in tensorflow written NHWC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.062782</td>\n",
       "      <td>0.041600</td>\n",
       "      <td>0.988714</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022793</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>0.991659</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2,0.01)\n",
    "# https://iconof.com/1cycle-learning-rate-policy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First convolutional layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (1): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = learn.model[0]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch has it built in. It's called `F.conv2d` (recall that `F` is a fastai import from `torch.nn.functional`, as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters:\n",
    "\n",
    "- input:: input tensor of shape `(minibatch, in_channels, iH, iW)`\n",
    "- weight:: filters of shape `(out_channels, in_channels, kH, kW)`\n",
    "\n",
    "Here `iH,iW` is the height and width of the image (i.e., `28,28`), and `kH,kW` is the height and width of our kernel (`3,3`). But apparently PyTorch is expecting rank-4 tensors for both these arguments, whereas currently we only have rank-2 tensors (i.e., matrices, or arrays with two axes).\n",
    "\n",
    "Kernels passed to `F.conv2d` need to be rank-4 tensors: `[channels_in, features_out, rows, columns]` , where channels_in is the number of input channels, and features_out is the number of output channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
